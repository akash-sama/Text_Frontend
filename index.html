<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Abusive Text Detection</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="styles.css" rel="stylesheet"> <!-- Assuming external CSS is named styles.css -->
</head>
<body>
    <div class="container shadow-lg p-3 mb-5 bg-white rounded">
        <h2 class="text-center">Akash's Text Detection Tool</h2>
        <p>Wait for the model to load before running</p>
         <small>This is frontend ML made with JavaScript and runs in the browser itself.</small>
        <div class="mb-3">
            <label for="textInput" class="form-label"><b>Enter text to check for abuse:</b></label>
            <textarea class="form-control" id="textInput" placeholder="Type your paragraph here..."></textarea>
        </div>
        <button class="btn btn-danger" onclick="detectAbuse()">Check for Abuse</button>
        <p id="modelStatus"></p>

        <p id="result" class="mt-3"></p>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/toxicity"></script>
    <script src="app.js"></script>
</body>
</html>
